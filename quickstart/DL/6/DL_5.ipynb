{"cells":[{"cell_type":"markdown","metadata":{"id":"ua4wuagXwmaJ"},"source":["# 05.  Transformers\n","\n","В этом нотбуке, мы рассмотрим реализацию Transformer с помощью PyTorch практически с нуля. Трансформер будет обучен для перевода текста из одного языка в другой."]},{"cell_type":"markdown","metadata":{"id":"Ge-0IvcN3UIB"},"source":["Через PyTorch будут реализованы все слои, необходимые трансформеру:\n","\n","1. [MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)\n","2. [FFN](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","3. [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n","4. [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n","5. [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpABGFRnUP62","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745423857535,"user_tz":-180,"elapsed":7672,"user":{"displayName":"Роман Гараев","userId":"04763128286090120144"}},"outputId":"efad7222-e548-4872-e852-d37aa962093c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.2)\n","Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"]}],"source":["# Для загрузки датасета пар предложений (англ,фр)\n","!pip install datasets\n","# Для подсчета метрики BLEU\n","!pip install sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxD4BEIMnUYz"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.optim as optim\n","import math"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-btisUnDdSCj","outputId":"32015e34-df8b-44cb-a277-cbd0621367cb","executionInfo":{"status":"ok","timestamp":1745423864814,"user_tz":-180,"elapsed":11,"user":{"displayName":"Роман Гараев","userId":"04763128286090120144"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{"id":"NB7MRv42BmuY"},"source":["## Positional Encoding"]},{"cell_type":"markdown","metadata":{"id":"hObFUSZJK6Rz"},"source":["PE (позиционная кодировка) - это метод, которым мы добавляем информацию о расположении слова в предложении."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CImV0MHb8PIo"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        # Initialize the PositionalEncoding class\n","        super(PositionalEncoding, self).__init__()\n","\n","        # Create a tensor to store positional encodings with shape (max_len, d_model)\n","        # max_len: maximum length of the input sequence\n","        # d_model: dimension of the model embeddings (matches token embedding size)\n","        self.encoding = torch.zeros(max_len, d_model)\n","\n","        # Create a tensor of positions from 0 to max_len-1 with shape (max_len, 1)\n","        # Each row represents the position of a word in the sequence\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","\n","        # Compute the denominator for the sine and cosine positional encodings\n","        # This creates a scaling factor based on the position in the model dimension (d_model)\n","        # d_model is divided by 2 since we alternate between sine and cosine encodings\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        # Apply sine to even dimensions (0, 2, 4, ...) of the positional encoding\n","        # This is done by multiplying the position by the div_term and applying sin\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)\n","\n","        # Apply cosine to odd dimensions (1, 3, 5, ...) of the positional encoding\n","        # This is done by multiplying the position by the div_term and applying cos\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)\n","\n","        # Add an extra dimension at the start to match the batch size during training\n","        # Shape becomes (1, max_len, d_model), allowing it to be added to input sequences\n","        self.encoding = self.encoding.unsqueeze(0)\n","\n","    def forward(self, x):\n","        # Add the positional encoding to the input tensor (x), ensuring that it is on the same device (CPU or GPU)\n","        # The encoding tensor is sliced to match the sequence length of the input (x.size(1))\n","        # This step adds positional information to the input embeddings\n","        x = x + self.encoding[:, :x.size(1), :].to(x.device)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"s89JutSTBpHr"},"source":["## Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGYT49wrFgPZ"},"outputs":[],"source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n","        super(TransformerEncoderLayer, self).__init__()\n","\n","        # Multi-Head Attention\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n","\n","        # Feedforward network: Linear -> ReLU -> Linear\n","        self.feedforward = nn.Sequential(\n","            nn.Linear(d_model, dim_feedforward),\n","            nn.ReLU(),\n","            nn.Linear(dim_feedforward, d_model)\n","        )\n","\n","        # Layer normalization and dropout\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n","        # Self-attention with residual connection and layer normalization\n","        attn_output, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n","        src = self.norm1(src + self.dropout1(attn_output))\n","\n","        # Feedforward network with residual connection and layer normalization\n","        feedforward_output = self.feedforward(src)\n","        src = self.norm2(src + self.dropout2(feedforward_output))\n","\n","        return src\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LH_NjP26FkGd"},"outputs":[],"source":["class TransformerDecoderLayer(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n","        super(TransformerDecoderLayer, self).__init__()\n","\n","        # Multi-Head Attention for target sequence\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n","\n","        # Multi-Head Attention for target attending to encoder output (memory)\n","        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n","\n","        # Feedforward network\n","        self.feedforward = nn.Sequential(\n","            nn.Linear(d_model, dim_feedforward),\n","            nn.ReLU(),\n","            nn.Linear(dim_feedforward, d_model)\n","        )\n","\n","        # Layer normalization and dropout\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.dropout3 = nn.Dropout(dropout)\n","\n","    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n","        # Self-attention for the target sequence\n","        attn_output, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)\n","        tgt = self.norm1(tgt + self.dropout1(attn_output))\n","\n","        # Cross-attention between target and memory (encoder output)\n","        attn_output, _ = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)\n","        tgt = self.norm2(tgt + self.dropout2(attn_output))\n","\n","        # Feedforward network\n","        feedforward_output = self.feedforward(tgt)\n","        tgt = self.norm3(tgt + self.dropout3(feedforward_output))\n","\n","        return tgt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgNAhmZNFofP"},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, encoder_layer, num_layers):\n","        super(TransformerEncoder, self).__init__()\n","        # Stack of Transformer Encoder layers\n","        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n","\n","    def forward(self, src, mask=None, src_key_padding_mask=None):\n","        # Pass the input through each encoder layer\n","        for layer in self.layers:\n","            src = layer(src, mask, src_key_padding_mask)\n","        return src\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JkuhaorYFq59"},"outputs":[],"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, decoder_layer, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        # Stack of Transformer Decoder layers\n","        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n","\n","    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n","        # Pass the input through each decoder layer\n","        for layer in self.layers:\n","            tgt = layer(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\n","        return tgt\n"]},{"cell_type":"markdown","metadata":{"id":"uN25j4f7NG10"},"source":["Here we implement the Transformer architecture in `nn.Module`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDa8aZagpjCq"},"outputs":[],"source":["class TransformerModel(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,        # Vocabulary size of the source language (input)\n","        tgt_vocab_size,        # Vocabulary size of the target language (output)\n","        d_model=512,           # Dimension of model embeddings (default 512)\n","        nhead=8,               # Number of attention heads in multi-head attention (default 8)\n","        num_encoder_layers=6,  # Number of layers in the Transformer encoder (default 6)\n","        num_decoder_layers=6,  # Number of layers in the Transformer decoder (default 6)\n","        dim_feedforward=2048,  # Dimension of the feedforward network inside the Transformer (default 2048)\n","        dropout=0.1            # Dropout rate (default 0.1)\n","    ):\n","        # Initialize the nn.Module parent class\n","        super(TransformerModel, self).__init__()\n","\n","        # Source embedding layer that converts input tokens to embeddings of size d_model\n","        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n","\n","        # Target embedding layer that converts target tokens to embeddings of size d_model\n","        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","\n","        # Positional encoding for the embeddings to encode the positions of tokens in the sequence\n","        self.positional_encoding = PositionalEncoding(d_model)\n","\n","        # Create a single layer of Transformer encoder\n","        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n","\n","        # Stack multiple encoder layers (num_encoder_layers defines how many)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n","\n","        # Create a single layer of Transformer decoder\n","        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n","\n","        # Stack multiple decoder layers (num_decoder_layers defines how many)\n","        self.transformer_decoder = TransformerDecoder(decoder_layer, num_decoder_layers)\n","\n","        # Final linear layer to map the decoder output to the target vocabulary size\n","        # The output dimension is the size of the target vocabulary (tgt_vocab_size)\n","        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n","\n","        # Store the model embedding dimension (d_model) for scaling later\n","        self.d_model = d_model\n","\n","    def forward(\n","        self,\n","        src,                    # Source sequence (input)\n","        tgt,                    # Target sequence (input for the decoder)\n","        src_mask,               # Mask for the source sequence (to avoid attending to padding tokens)\n","        tgt_mask,               # Mask for the target sequence (prevents attention to future tokens)\n","        src_padding_mask,       # Padding mask for the source (to avoid attention to padding)\n","        tgt_padding_mask,       # Padding mask for the target (to avoid attention to padding)\n","        memory_key_padding_mask # Padding mask for the memory (encoder output) in the decoder\n","    ):\n","        # Embed the source sequence and scale by sqrt of d_model for stable gradients\n","        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n","\n","        # Embed the target sequence and scale by sqrt of d_model\n","        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n","\n","        # Add positional encodings to the source embeddings\n","        src_emb = self.positional_encoding(src_emb)\n","\n","        # Add positional encodings to the target embeddings\n","        tgt_emb = self.positional_encoding(tgt_emb)\n","\n","        # Pass the source embeddings through the Transformer encoder\n","        # The encoder produces a memory representation for the source sequence\n","        memory = self.transformer_encoder(src_emb, mask=src_mask, src_key_padding_mask=src_padding_mask)\n","\n","        # Pass the target embeddings and the memory (encoder output) through the Transformer decoder\n","        # The decoder attends to both the target sequence and the encoder's memory\n","        output = self.transformer_decoder(\n","            tgt_emb, memory, tgt_mask=tgt_mask, memory_mask=src_mask,\n","            tgt_key_padding_mask=tgt_padding_mask,\n","            memory_key_padding_mask=memory_key_padding_mask\n","        )\n","\n","\n","        # Apply the final linear layer to map the decoder output to the target vocabulary\n","        output = self.fc_out(output)\n","\n","        # Return the final output (logits over the target vocabulary)\n","        return output\n"]},{"cell_type":"markdown","metadata":{"id":"VBzHTdq2BwDS"},"source":["## Mask Generation Functions"]},{"cell_type":"markdown","metadata":{"id":"pJgKfWGZO27c"},"source":["It is also necessary to implement functions that create masks for the input data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Io2Y6MEPByG9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745425048677,"user_tz":-180,"elapsed":14,"user":{"displayName":"Роман Гараев","userId":"04763128286090120144"}},"outputId":"eccaa17a-31f2-4a3e-d7fc-e4366a218ba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., -inf, -inf, -inf],\n","        [0., 0., -inf, -inf],\n","        [0., 0., 0., -inf],\n","        [0., 0., 0., 0.]])\n"]}],"source":["def generate_square_subsequent_mask(sz):\n","    # Create a square matrix of size (sz, sz) filled with ones above and on the diagonal, zeros below the diagonal.\n","    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","\n","    # Convert the mask to a float tensor and fill the zeros with -inf (indicating they should be ignored in attention)\n","    # and ones with 0.0 (indicating those positions can be attended to).\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","\n","    # Return the final mask\n","    return mask\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WkSP0DQPhQ6"},"outputs":[],"source":["def create_padding_mask(seq, pad_idx):\n","    # Return a boolean mask where the elements in the sequence that are equal to the pad_idx (padding index) are True,\n","    # indicating that those positions are padding tokens.\n","    return (seq == pad_idx).float()\n"]},{"cell_type":"markdown","metadata":{"id":"cymWwH0hC913"},"source":["## Machine Translation"]},{"cell_type":"markdown","metadata":{"id":"eX3Q4zwIP4ns"},"source":["After we have defined the model, now we can train the model to translate the text from English to French.\n","\n","So, let's start with loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGfu2acNDYiu"},"outputs":[],"source":["from datasets import load_dataset\n","\n","texts = load_dataset(\"opus_books\", \"en-fr\", split='train')"]},{"cell_type":"markdown","metadata":{"id":"mCq7jssjQXSM"},"source":["Here is just some unwrapping of the data. Nothing that important."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS9yuOoctt59"},"outputs":[],"source":["src_lang = 'en'\n","tgt_lang = 'fr'\n","\n","texts = texts.map(\n","    lambda x: {\n","        'en': x['translation']['en'],\n","        'fr': x['translation']['fr']\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"id":"AYVf6Ll4QjBs"},"source":["Definition of how many data samples should be used for training and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TEiGsBboBAV"},"outputs":[],"source":["# Total number of samples\n","max_size = len(texts)\n","# max_size = 10000\n","\n","# Percentage of samples from the total number that will be used for training\n","train_frac = 0.8 # Percen\n","\n","# Calculation of exact number of training and testing samples\n","train_size = int(train_frac * max_size)\n","test_size = max_size - train_size\n","\n","# Selection of the data for training and testing according to the number\n","# of training and testing samples\n","train_texts = texts.select(range(train_size))\n","test_texts = texts.select(range(train_size, train_size+test_size))"]},{"cell_type":"markdown","metadata":{"id":"zZ6GFWtKTC2j"},"source":["Wrap the dataset into PyTorch dataset in order to load data with DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qjgnaroH6Ht"},"outputs":[],"source":["class TranslationDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, src_lang='en', tgt_lang='fr'):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"src\": self.data[idx][src_lang],\n","            \"tgt\": self.data[idx][tgt_lang]\n","        }\n","\n","train_dataset = TranslationDataset(train_texts)\n","test_dataset = TranslationDataset(test_texts)"]},{"cell_type":"markdown","metadata":{"id":"ElCzpkf5V5d7"},"source":["For the sake of simplicity, we will use the ready-to-use tokenizer of English and French texts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3JX31ChFCpl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745425308790,"user_tz":-180,"elapsed":343,"user":{"displayName":"Роман Гараев","userId":"04763128286090120144"}},"outputId":"985a59d2-2132-4031-88bc-55c6a4e7c86d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]},{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":26}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n","\n","# Add Begining of Sentence (BOS) token since it is absent in the tokenizer\n","tokenizer.add_special_tokens({'bos_token': '<BOS>'})"]},{"cell_type":"markdown","metadata":{"id":"Ejpb030dWfa0"},"source":["Create training and testing DataLoaders. Implement `collate_fn` in which we tokenize the batch of text using [`tokenizer.batch_encode_plus`](https://huggingface.co/docs/transformers/v4.45.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7hHGwJzIZVs"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# Define the maximum sequence length and batch size\n","max_length = 64\n","batch_size = 64\n","\n","# Custom function to process and batch data\n","def collate_fn(batch):\n","    # Prepend \"<BOS>\" token to each source text in the batch and tokenize it\n","    tokenized_src_texts = tokenizer.batch_encode_plus(\n","        [\"<BOS> \" + item['src'] for item in batch],  # Add \"<BOS>\" to the source text before tokenization\n","        padding=\"max_length\",                        # Pad sequences to the max_length\n","        max_length=max_length,                       # Define the maximum length for padding/truncation\n","        truncation=True,                             # Truncate sequences longer than max_length\n","        return_tensors=\"pt\"                          # Return a PyTorch tensor\n","    )['input_ids']                                   # Extract the 'input_ids' (token IDs)\n","\n","    # Prepend \"<BOS>\" token to each target text in the batch and tokenize it\n","    tokenized_tgt_texts = tokenizer.batch_encode_plus(\n","        [\"<BOS> \" + item['tgt'] for item in batch],  # Add \"<BOS>\" to the target text before tokenization\n","        padding=\"max_length\",                        # Pad sequences to the max_length\n","        max_length=max_length,                       # Define the maximum length for padding/truncation\n","        truncation=True,                             # Truncate sequences longer than max_length\n","        return_tensors=\"pt\"                          # Return a PyTorch tensor\n","    )['input_ids']                                   # Extract the 'input_ids' (token IDs)\n","\n","    # Return the tokenized and padded source and target texts as tensors\n","    return {\n","        'src': tokenized_src_texts,                  # Tokenized source sequences tensor\n","        'tgt': tokenized_tgt_texts                   # Tokenized target sequences tensor\n","    }\n","\n","# Create DataLoader for training data\n","train_dataloader = DataLoader(\n","    train_dataset,                                   # Training dataset\n","    batch_size=batch_size,                           # Batch size for training\n","    collate_fn=collate_fn,                           # Custom collate function for tokenizing and batching\n","    shuffle=True                                     # Shuffle the data each epoch\n",")\n","\n","# Create DataLoader for test data\n","test_dataloader = DataLoader(\n","    test_dataset,                                    # Test dataset\n","    batch_size=batch_size,                           # Batch size for testing\n","    collate_fn=collate_fn,                           # Custom collate function for tokenizing and batching\n","    shuffle=False                                    # Do not shuffle the test data\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"aaJKVet2X5CF"},"source":["Create an instance of the model, optimizer, and loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkO46EkUKrR3"},"outputs":[],"source":["import copy\n","# Retrieve special token indices from the tokenizer\n","pad_idx = tokenizer.pad_token_id  # Padding token index\n","bos_idx = tokenizer.bos_token_id  # Beginning of sequence token index\n","eos_idx = tokenizer.eos_token_id  # End of sequence token index\n","\n","# Define vocabulary size, adding 1 due to the new BOS token\n","vocab_size = tokenizer.vocab_size + 1\n","\n","# Initialize the Transformer model\n","model = TransformerModel(\n","    vocab_size,               # Source vocabulary size (input)\n","    vocab_size,               # Target vocabulary size (output)\n","    d_model=512,              # Dimensionality of the model (embedding size)\n","    nhead=8,                  # Number of attention heads in the multi-head attention mechanism\n","    num_encoder_layers=6,     # Number of layers in the encoder\n","    num_decoder_layers=6,     # Number of layers in the decoder\n","    dim_feedforward=2048,     # Dimension of the feedforward network inside each Transformer layer\n","    dropout=0.1               # Dropout rate for regularization\n",").to(device)                  # Move the model to the specified device (e.g., GPU or CPU)\n","\n","# Initialize the optimizer (Adam optimizer with a learning rate of 0.0001)\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","# Define the loss function (Cross-Entropy Loss) with the padding token ignored\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"]},{"cell_type":"markdown","metadata":{"id":"UyPAZvRkYcED"},"source":["Check how many parameters our model has"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6w9MHXJAYaA0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745424030306,"user_tz":-180,"elapsed":47,"user":{"displayName":"Роман Гараев","userId":"04763128286090120144"}},"outputId":"92ea9f56-93df-48cf-9620-94160f7ff7a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["135613051"]},"metadata":{},"execution_count":20}],"source":["sum(p.numel() for p in model.parameters())"]},{"cell_type":"markdown","metadata":{"id":"fda8qUv2Y1HN"},"source":["Define the function for training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyChRVn6MfSE","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"error","timestamp":1745424037723,"user_tz":-180,"elapsed":5864,"user":{"displayName":"Роман Гараев","userId":"04763128286090120144"}},"outputId":"88b5b91a-53c6-445d-d5a1-99ac537b836f"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loss: 10.478537559509277:   0%|          | 6/1589 [00:05<24:55,  1.06it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-1381bd991f27>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Call the training function with one epoch of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# The Transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# DataLoader for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-1381bd991f27>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, pad_idx, device, num_epochs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update the model's parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Accumulate total loss for this epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Update the progress bar description with the current average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tqdm import tqdm  # Import tqdm for progress bars\n","\n","def train_model(\n","    model,                # The Transformer model\n","    train_loader,         # DataLoader for the training data\n","    optimizer,            # Optimizer (Adam in this case)\n","    criterion,            # Loss function (Cross-Entropy Loss)\n","    pad_idx,              # Padding token index to ignore in loss calculation\n","    device,               # Device (CPU or GPU)\n","    num_epochs=10         # Number of epochs to train the model\n","):\n","    model.train()  # Set the model to training mode\n","    for epoch in range(num_epochs):  # Loop through each epoch\n","        total_loss = 0  # Initialize total loss for the current epoch\n","\n","        pbar = tqdm(train_loader)  # Wrap train_loader with tqdm for progress display\n","        for i, batch in enumerate(pbar):  # Loop through each batch in the training data\n","            # Move the source and target sequences to the device (e.g., GPU)\n","            src, tgt = batch['src'].to(device), batch['tgt'].to(device)\n","\n","            # Prepare input and output sequences for teacher forcing\n","            tgt_input = tgt[:, :-1]  # Target input (remove the last token)\n","            tgt_output = tgt[:, 1:]  # Target output (shifted by one token)\n","\n","            # Generate masks for attention\n","            src_mask = None  # No source mask in this case\n","            tgt_mask = generate_square_subsequent_mask(  # Generate mask for target input to prevent attending to future tokens\n","                tgt_input.size(1)\n","            ).to(device)\n","\n","            # Create padding masks for both source and target sequences\n","            src_padding_mask = create_padding_mask(src, pad_idx).to(device)\n","            tgt_padding_mask = create_padding_mask(tgt_input, pad_idx).to(device)\n","            memory_padding_mask = create_padding_mask(src, pad_idx).to(device)\n","\n","            optimizer.zero_grad()  # Zero gradients before backpropagation\n","\n","            # Forward pass through the model\n","            output = model(\n","                src, tgt_input, src_mask, tgt_mask,  # Source and target with their masks\n","                src_padding_mask, tgt_padding_mask, memory_padding_mask  # Padding masks for the attention mechanism\n","            )\n","            output = output.view(-1, output.size(-1))  # Reshape the model's output for calculating loss\n","            tgt_output = tgt_output.contiguous().view(-1)  # Flatten the target output for loss calculation\n","\n","            # Compute the loss\n","            loss = criterion(output, tgt_output)  # Cross-entropy loss\n","            loss.backward()  # Backpropagate the gradients\n","            optimizer.step()  # Update the model's parameters\n","\n","            total_loss += loss.item()  # Accumulate total loss for this epoch\n","\n","            # Update the progress bar description with the current average loss\n","            pbar.set_description(f\"Loss: {total_loss / (i + 1)}\")\n","\n","\n","# Call the training function with one epoch of training\n","train_model(\n","    model,                 # The Transformer model\n","    train_dataloader,      # DataLoader for training data\n","    optimizer,             # Adam optimizer\n","    criterion,             # Cross-Entropy Loss\n","    pad_idx,               # Padding index to ignore in the loss\n","    device,                # Device (GPU or CPU)\n","    num_epochs=1           # Number of epochs to train\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"MbbMRz9nZvis"},"source":["Check how the model translates the English text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3W_uNjAwMYN"},"outputs":[],"source":["def translate_sentence(model, tokenizer, sentence, max_length, pad_idx, device):\n","    # Tokenize the input sentence\n","    model.eval()  # Set the model to evaluation mode\n","    tokens = tokenizer.encode(sentence, return_tensors=\"pt\").to(device)\n","\n","    # Ensure tokens are not longer than max_length\n","    tokens = tokens[:, :max_length]\n","\n","    # Prepare the input tensor (add batch dimension if necessary)\n","    src = tokens\n","\n","    # Initialize the decoder input with <BOS> token\n","    tgt = torch.tensor([[bos_idx]], dtype=torch.long).to(device)\n","\n","    # Create padding masks\n","    src_padding_mask = create_padding_mask(src, pad_idx).to(device)\n","\n","    # Generate a translation by decoding one token at a time\n","    for _ in range(max_length):\n","\n","        with torch.no_grad():\n","            output = model(\n","                src, tgt, src_mask=None, tgt_mask=None,\n","                src_padding_mask=src_padding_mask, tgt_padding_mask=None,\n","                memory_key_padding_mask=src_padding_mask\n","            )\n","\n","        # Get the last predicted token (greedy decoding)\n","        next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(0)\n","\n","        # Concatenate predicted token to target sequence\n","        tgt = torch.cat([tgt, next_token], dim=1)\n","\n","        # Stop if <EOS> token is predicted\n","        if next_token.item() == eos_idx:\n","            break\n","\n","    # Decode the target tokens back into text\n","    translated_tokens = tgt.squeeze().tolist()\n","\n","    # Convert token IDs back to the target language sentence\n","    translated_sentence = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n","\n","    return translated_sentence\n","\n","\n","# Test translation example\n","for example_idx in range(5):\n","\n","    example_sentence = test_texts[example_idx]['en']\n","    refence_sentence = test_texts[example_idx]['fr']\n","    translated_sentence = translate_sentence(\n","        model, tokenizer, example_sentence, max_length=max_length, pad_idx=pad_idx, device=device\n","    )\n","\n","    print(f\"Source sentence: {example_sentence}\")\n","    print(f\"Translated sentence: {translated_sentence}\")\n","    print(f\"Reference sentence: {refence_sentence}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"Ji_s-849Z5D-"},"source":["Calculate the metrics of translation quality: [BLEU](https://en.wikipedia.org/wiki/BLEU) score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5-k-7FumP61"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import sacrebleu\n","\n","# Function to evaluate the model on the test set\n","def evaluate_model(model, test_loader, criterion, pad_idx, device):\n","    model.eval()\n","    total_loss = 0\n","    all_references = []\n","    all_hypotheses = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(test_loader):\n","            src, tgt = batch['src'].to(device), batch['tgt'].to(device)\n","\n","            # Prepare input and output sequences\n","            tgt_input = tgt[:, :-1]  # Input to the decoder\n","            tgt_output = tgt[:, 1:]  # Target to compare with output\n","\n","            src_mask = None\n","            tgt_mask = None\n","\n","            src_padding_mask = create_padding_mask(src, pad_idx).to(device)\n","            tgt_padding_mask = create_padding_mask(tgt_input, pad_idx).to(device)\n","            memory_padding_mask = create_padding_mask(src, pad_idx).to(device)\n","\n","            # Forward pass\n","            output = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_padding_mask)\n","\n","            # Compute the loss\n","            output_flat = output.view(-1, output.size(-1))\n","            tgt_output_flat = tgt_output.contiguous().view(-1)\n","            loss = criterion(output_flat, tgt_output_flat)\n","            total_loss += loss.item()\n","\n","            # Generate predictions (greedy decoding)\n","            decoded_sentences = torch.argmax(output, dim=-1)\n","\n","            # Convert decoded sentences to strings and store for BLEU score calculation\n","            for i in range(decoded_sentences.size(0)):\n","                # Remove padding tokens, BOS, and EOS tokens from the predictions\n","                pred_tokens = decoded_sentences[i].tolist()\n","                pred_tokens = [token for token in pred_tokens if token != pad_idx]\n","\n","                # Convert tokens back to string\n","                hypothesis = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n","\n","                # Store the generated sentence (hypothesis)\n","                all_hypotheses.append(hypothesis)\n","\n","                # Convert target tokens to string\n","                target_tokens = tgt[i, 1:].tolist()  # Remove <BOS>\n","                target_tokens = [token for token in target_tokens if token != pad_idx]\n","\n","                reference = tokenizer.decode(target_tokens, skip_special_tokens=True)\n","\n","                # Store the reference sentence\n","                all_references.append([reference])  # sacrebleu expects a list of references\n","\n","    # Compute BLEU score\n","    bleu = sacrebleu.corpus_bleu(all_hypotheses, all_references)\n","    avg_loss = total_loss / len(test_loader)\n","\n","    print(f\"Test Loss: {avg_loss}\")\n","    print(f\"BLEU Score: {bleu.score}\")\n","\n","    return avg_loss, bleu.score\n","\n","\n","# Running the evaluation\n","evaluate_model(model, test_dataloader, criterion, pad_idx, device)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}