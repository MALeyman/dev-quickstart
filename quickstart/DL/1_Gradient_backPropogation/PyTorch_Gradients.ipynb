{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLv6YV8s6w-V"
      },
      "source": [
        "# Lab 01. Градиенты в Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGORocxJ6w-a"
      },
      "source": [
        "## Autograd - Автоматическое дифференцирование\n",
        "\n",
        "В первой лабораторной работе, мы рассмотрим концепцию динамического вычислительного графа, который состоит из всех объектов Tensor, связанных в сеть, а также функций, используемых для их создания. Обратите внимание, что только те входные тензоры, которые мы создаем сами, не будут иметь связанных объектов Function.\n",
        "\n",
        "Пакет autograd из PyTorch обеспечивает автоматическое дифференциорование для всех операций на Tensors. Операции являются атрибутами самих тензоров.\n",
        "\n",
        "Когда для атрибута .requires_grad у Tensor  установлено значение True, он начинает отслеживать все операции над ним. Когда операция заканчивается, вы можете вызвать .backward () и автоматически рассчитать все градиенты. Градиент для тензора будет накапливаться в его атрибуте .grad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gUy-Vut6w-b"
      },
      "source": [
        "## Backpropagation за один шаг\n",
        "Мы начнем с применения одной полиномиальной функции $y = f(x)$ к тензору $x$. Затем выполним backprop для подсчета $\\frac {dy} {dx}$.\n",
        "\n",
        "$$\\quad y= 2x^4 + x^3 + 3x^2 + 5x + 1$$\n",
        "\n",
        "$$\\quad y'= 8x^3 + 3x^2 + 6x + 5$$\n",
        "\n",
        "#### Step 1. Импорт"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bdok4Usa6w-c"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00-jMjdW6w-d"
      },
      "source": [
        "#### Step 2. Создаем тензор. <tt>requires_grad</tt> устанавливаем в  True\n",
        "Это устанавливает отслеживание вычислений на тензоре."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DRSiXGK16w-d"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tJMTzRLq6w-d",
        "outputId": "f156f073-3ff8-418d-ac82-b0044f9afc1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adz5JhM06w-e"
      },
      "source": [
        "#### Step 3. Задаем функцию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gnwMj0t06w-e",
        "outputId": "0ec44222-e0ee-4676-fe50-8f4115c7a7ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(63., grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
        "\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77A4Tlww6w-e"
      },
      "source": [
        "Поскольку $y$ была создана в результате операции, у нее есть связанная функция градиента, доступная через <tt> y.grad_fn </tt> <br>\n",
        "Вычисление $y$ для данного $x=2$ выполняется следующим образом: <br>\n",
        "\n",
        "$\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$\n",
        "\n",
        "Значение $y=63$ при $x=2$.\n",
        "\n",
        "#### Step 4. Backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yw8It4-H6w-f"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rVPPOB76w-f"
      },
      "source": [
        "#### Step 5. Показать полученный градиент"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e_IesNPO6w-f",
        "outputId": "83c08d0d-c4b6-4df5-ed5f-69aba60081d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(93.)\n"
          ]
        }
      ],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpBFkrqz6w-f"
      },
      "source": [
        "Обратите внимание, что <tt> x.grad </tt> является атрибутом тензора $x$, поэтому мы не используем скобки. Вычисление является результатом <br>\n",
        "\n",
        "$ \\quad y'= 8 (2)^3 + 3 (2)^2 + 6 (2) +5 = 64 + 12 + 12 + 5 = 93 $\n",
        "\n",
        "Это значение производной в точке $ (2,63) $.\n",
        "\n",
        "## Back-propagation с несколькими шагами\n",
        "Теперь давайте сделаем что-то более сложное, включив слои $ y $ и $ z $ между $ x $ и нашим выходным слоем $ out $.\n",
        "#### 1. Создаем тензор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EXa_9Xev6w-f",
        "outputId": "e6a86733-5bdd-42a7-d8c2-0d3449400b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [3., 2., 1.]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1.0, 2.0, 3.0],[3.0, 2.0, 1.0]], requires_grad=True)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpiHmUAT6w-g"
      },
      "source": [
        "#### 2. Создаем первый слой $y = 3x+2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_OBIdMZJ6w-g",
        "outputId": "f2859486-817a-478d-f885-6f95b289c5a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 5.,  8., 11.],\n",
            "        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "y = 3*x + 2\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6oZKBim6w-g"
      },
      "source": [
        "#### 3. Создаем второй слой $z = 2y^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1GsfCXVC6w-g",
        "outputId": "3defe1c0-469b-4f67-dd4b-4164da629475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 50., 128., 242.],\n",
            "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "z = 2*y**2\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWjwCzVj6w-g"
      },
      "source": [
        "#### 4. Выходное значение $out$ считаем как среднее"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-hlHFybC6w-h",
        "outputId": "7d20eb0c-f16b-4d13-9032-af871b1c7339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(140., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "out = z.mean()\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raK--6Wk6w-h"
      },
      "source": [
        "#### 5. Выполняем back-propagation чтобы найти градиент от $out$ по x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5hkpPkgl6w-h",
        "outputId": "fb5219d7-61c3-43a0-876f-c1873558746f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[10., 16., 22.],\n",
            "        [22., 16., 10.]])\n"
          ]
        }
      ],
      "source": [
        "out.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWugZ6HY6w-h"
      },
      "source": [
        "Вы должны увидеть матрицу 2х3. Если мы обозначим финальный тензор <tt> out </tt> через «$ o $», мы можем вычислить частную производную $ o $ по $ x_i $ следующим образом: <br>\n",
        "\n",
        "$o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$<br>\n",
        "\n",
        "$z_i = 2(y_i)^2 = 2(3x_i+2)^2$<br>\n",
        "\n",
        "Для решения производной от $ z_i $ мы используем chain rule (дифференцирование сложной функции), то есть производная от $f(g(x)) = f'(g(x))g'(x)$<br>\n",
        "\n",
        "В этом случае<br>\n",
        "\n",
        "$\\begin{split} f(g(x)) &= 2(g(x))^2, \\quad &f'(g(x)) = 4g(x) \\\\\n",
        "g(x) &= 3x+2, &g'(x) = 3 \\\\\n",
        "\\frac {dz} {dx} &= 4g(x)\\times 3 &= 12(3x+2) \\end{split}$\n",
        "\n",
        "Поэтому,<br>\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$<br>\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCtRJoEN6w-h"
      },
      "source": [
        "### Отключить отслеживание градиента\n",
        "Могут быть случаи, когда не нужно отслеживать историю вычислений.\n",
        "\n",
        "Вы можете выставить  атрибут <tt> require_grad </tt>, используя `.requires_grad_ (True)` (или False) по мере необходимости.\n",
        "\n",
        "При выполнении вычислений часто полезно обернуть набор операций в блок `with torch.no_grad(): {...}`\n",
        "\n",
        "\n",
        "Другой метод - запуск `.detach()` для тензора, чтобы предотвратить отслеживание будущих вычислений. Это может быть удобно при клонировании тензора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "l7LUPOHgQdfb",
        "outputId": "f87663de-cdf1-4d3d-a27e-fdd04e7e44bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 5.,  8., 11.],\n",
            "        [11.,  8.,  5.]])\n"
          ]
        }
      ],
      "source": [
        "x.requires_grad = False\n",
        "y = 3*x + 2\n",
        "# backward вызовет ошибку, тк подсчет градиентов отключен\n",
        "#y.backward()\n",
        "print(y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
